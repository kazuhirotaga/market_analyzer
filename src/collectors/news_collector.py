"""ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿åŽé›†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

NewsAPI, NewsData.io, marketaux ã‚’ä½¿ç”¨ã—ã¦ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’å–å¾—ã™ã‚‹ã€‚
"""

import logging
import hashlib
from datetime import datetime, timedelta
from typing import Optional

import requests

from src.config import config
from src.database.models import get_session, NewsArticle

logger = logging.getLogger(__name__)


class NewsCollector:
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹åŽé›†ã‚¯ãƒ©ã‚¹"""

    NEWSAPI_BASE = "https://newsapi.org/v2"
    NEWSDATA_BASE = "https://newsdata.io/api/1"
    MARKETAUX_BASE = "https://api.marketaux.com/v1"

    def __init__(self):
        self.api_keys = config.api_keys
        self.keywords = config.news_keywords
        self._seen_hashes: set[str] = set()

    def collect_all(self) -> list[dict]:
        """å…¨ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åŽé›†"""
        all_articles = []

        # NewsAPI
        if self.api_keys.newsapi:
            articles = self._collect_from_newsapi()
            all_articles.extend(articles)
            logger.info(f"ðŸ“° NewsAPI: {len(articles)} ä»¶å–å¾—")
        else:
            logger.warning("âš ï¸ NewsAPI ã‚­ãƒ¼ãŒæœªè¨­å®šã§ã™")

        # NewsData.io
        if self.api_keys.newsdata:
            articles = self._collect_from_newsdata()
            all_articles.extend(articles)
            logger.info(f"ðŸ“° NewsData.io: {len(articles)} ä»¶å–å¾—")
        else:
            logger.warning("âš ï¸ NewsData ã‚­ãƒ¼ãŒæœªè¨­å®šã§ã™")

        # marketaux
        if self.api_keys.marketaux:
            articles = self._collect_from_marketaux()
            all_articles.extend(articles)
            logger.info(f"ðŸ“° marketaux: {len(articles)} ä»¶å–å¾—")
        else:
            logger.warning("âš ï¸ marketaux ã‚­ãƒ¼ãŒæœªè¨­å®šã§ã™")

        # é‡è¤‡æŽ’é™¤
        unique_articles = self._deduplicate(all_articles)
        logger.info(f"ðŸ“° ãƒ‹ãƒ¥ãƒ¼ã‚¹åŽé›†å®Œäº†: {len(unique_articles)} ä»¶ (é‡è¤‡æŽ’é™¤å¾Œ)")

        # DBä¿å­˜
        saved = self._save_articles(unique_articles)
        logger.info(f"ðŸ’¾ DBä¿å­˜: {saved} ä»¶")

        return unique_articles

    def collect_for_category(self, category: str) -> list[dict]:
        """æŒ‡å®šã‚«ãƒ†ã‚´ãƒªã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åŽé›†"""
        keywords = self.keywords.get(category, [])
        if not keywords:
            logger.warning(f"âš ï¸ ã‚«ãƒ†ã‚´ãƒª '{category}' ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒæœªå®šç¾©ã§ã™")
            return []

        articles = []
        query = " OR ".join(keywords)

        if self.api_keys.newsapi:
            articles.extend(self._search_newsapi(query, category))

        unique_articles = self._deduplicate(articles)
        self._save_articles(unique_articles)
        return unique_articles

    # --- NewsAPI ---

    def _collect_from_newsapi(self) -> list[dict]:
        """NewsAPIã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åŽé›†"""
        articles = []
        for category, keywords in self.keywords.items():
            query = " OR ".join(keywords[:5])  # APIã®ã‚¯ã‚¨ãƒªé•·åˆ¶é™
            articles.extend(self._search_newsapi(query, category))
        return articles

    def _search_newsapi(self, query: str, category: str) -> list[dict]:
        """NewsAPIã§ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢"""
        try:
            params = {
                "q": query,
                "language": "jp",
                "sortBy": "publishedAt",
                "pageSize": 20,
                "apiKey": self.api_keys.newsapi,
            }

            response = requests.get(
                f"{self.NEWSAPI_BASE}/everything",
                params=params,
                timeout=15,
            )
            response.raise_for_status()
            data = response.json()

            articles = []
            for item in data.get("articles", []):
                articles.append({
                    "title": item.get("title", ""),
                    "content": item.get("description", "") or item.get("content", ""),
                    "url": item.get("url", ""),
                    "source": f"newsapi:{item.get('source', {}).get('name', 'unknown')}",
                    "category": category,
                    "published_at": self._parse_datetime(item.get("publishedAt")),
                })

            return articles
        except Exception as e:
            logger.warning(f"âš ï¸ NewsAPIæ¤œç´¢ã‚¨ãƒ©ãƒ¼ ({category}): {e}")
            return []

    # --- NewsData.io ---

    def _collect_from_newsdata(self) -> list[dict]:
        """NewsData.ioã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åŽé›†"""
        articles = []
        for category, keywords in self.keywords.items():
            query = " OR ".join(keywords[:3])
            try:
                params = {
                    "apikey": self.api_keys.newsdata,
                    "q": query,
                    "country": "jp",
                    "language": "ja",
                    "category": "business",
                }

                response = requests.get(
                    f"{self.NEWSDATA_BASE}/latest",
                    params=params,
                    timeout=15,
                )
                response.raise_for_status()
                data = response.json()

                for item in data.get("results", []):
                    articles.append({
                        "title": item.get("title", ""),
                        "content": item.get("description", "") or item.get("content", ""),
                        "url": item.get("link", ""),
                        "source": f"newsdata:{item.get('source_id', 'unknown')}",
                        "category": category,
                        "published_at": self._parse_datetime(item.get("pubDate")),
                    })

            except Exception as e:
                logger.warning(f"âš ï¸ NewsDataæ¤œç´¢ã‚¨ãƒ©ãƒ¼ ({category}): {e}")

        return articles

    # --- marketaux ---

    def _collect_from_marketaux(self) -> list[dict]:
        """marketauxã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åŽé›†"""
        articles = []
        try:
            # æ—¥æœ¬å¸‚å ´é–¢é€£ã®éŠ˜æŸ„ã‚·ãƒ³ãƒœãƒ«ã‚’å¯¾è±¡
            params = {
                "api_token": self.api_keys.marketaux,
                "countries": "jp",
                "filter_entities": "true",
                "limit": 50,
            }

            response = requests.get(
                f"{self.MARKETAUX_BASE}/news/all",
                params=params,
                timeout=15,
            )
            response.raise_for_status()
            data = response.json()

            for item in data.get("data", []):
                # marketauxã«ã¯ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆãŒå«ã¾ã‚Œã‚‹å ´åˆãŒã‚ã‚‹
                sentiment = None
                entities = item.get("entities", [])
                if entities:
                    sentiments = [
                        e.get("sentiment_score") for e in entities
                        if e.get("sentiment_score") is not None
                    ]
                    if sentiments:
                        sentiment = sum(sentiments) / len(sentiments)

                articles.append({
                    "title": item.get("title", ""),
                    "content": item.get("description", "") or item.get("snippet", ""),
                    "url": item.get("url", ""),
                    "source": f"marketaux:{item.get('source', 'unknown')}",
                    "category": "stock",
                    "published_at": self._parse_datetime(item.get("published_at")),
                    "sentiment_score": sentiment,
                    "model_used": "marketaux" if sentiment is not None else None,
                })

        except Exception as e:
            logger.warning(f"âš ï¸ marketauxæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")

        return articles

    # --- ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ---

    def _deduplicate(self, articles: list[dict]) -> list[dict]:
        """ã‚¿ã‚¤ãƒˆãƒ«ãƒ™ãƒ¼ã‚¹ã§é‡è¤‡æŽ’é™¤"""
        unique = []
        for article in articles:
            title = article.get("title", "")
            if not title:
                continue
            h = hashlib.md5(title.encode("utf-8")).hexdigest()
            if h not in self._seen_hashes:
                self._seen_hashes.add(h)
                unique.append(article)
        return unique

    def _parse_datetime(self, dt_str: Optional[str]) -> Optional[datetime]:
        """æ—¥æ™‚æ–‡å­—åˆ—ã‚’ãƒ‘ãƒ¼ã‚¹"""
        if not dt_str:
            return None
        formats = [
            "%Y-%m-%dT%H:%M:%SZ",
            "%Y-%m-%dT%H:%M:%S.%fZ",
            "%Y-%m-%d %H:%M:%S",
            "%Y-%m-%d",
        ]
        for fmt in formats:
            try:
                return datetime.strptime(dt_str, fmt)
            except (ValueError, TypeError):
                continue
        return None

    def _save_articles(self, articles: list[dict]) -> int:
        """è¨˜äº‹ã‚’DBã«ä¿å­˜"""
        session = get_session()
        saved = 0
        try:
            for article in articles:
                title = article.get("title", "")
                if not title:
                    continue

                # æ—¢å­˜ãƒã‚§ãƒƒã‚¯ (ã‚¿ã‚¤ãƒˆãƒ« + ã‚½ãƒ¼ã‚¹ã§é‡è¤‡åˆ¤å®š)
                existing = (
                    session.query(NewsArticle)
                    .filter_by(title=title, source=article.get("source", ""))
                    .first()
                )
                if existing:
                    continue

                news = NewsArticle(
                    title=title,
                    content=article.get("content"),
                    url=article.get("url"),
                    source=article.get("source"),
                    category=article.get("category"),
                    published_at=article.get("published_at"),
                    sentiment_score=article.get("sentiment_score"),
                    confidence=article.get("confidence"),
                    model_used=article.get("model_used"),
                )
                session.add(news)
                saved += 1

            session.commit()
        except Exception as e:
            session.rollback()
            logger.error(f"âŒ è¨˜äº‹ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        finally:
            session.close()

        return saved

    def get_recent_articles(
        self,
        category: Optional[str] = None,
        days: int = 7,
        limit: int = 100,
    ) -> list[dict]:
        """DBã‹ã‚‰ç›´è¿‘Næ—¥é–“ã®è¨˜äº‹ã‚’å–å¾—"""
        session = get_session()
        try:
            cutoff = datetime.utcnow() - timedelta(days=days)
            query = session.query(NewsArticle).filter(
                NewsArticle.collected_at >= cutoff
            )

            if category:
                query = query.filter(NewsArticle.category == category)

            query = query.order_by(NewsArticle.published_at.desc()).limit(limit)
            rows = query.all()

            return [{
                "id": r.id,
                "title": r.title,
                "content": r.content,
                "url": r.url,
                "source": r.source,
                "category": r.category,
                "published_at": r.published_at.isoformat() if r.published_at else None,
                "sentiment_score": r.sentiment_score,
                "confidence": r.confidence,
                "model_used": r.model_used,
            } for r in rows]
        finally:
            session.close()
